{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from DrawtexDataset import DrawtexDataset\n",
    "from DrawtexModel import DrawTexModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SZ = 128\n",
    "TEST_BATCH_SZ = 1000\n",
    "EPOCHS = 1\n",
    "LEARN_RATE = 0.01\n",
    "CLASS_CNT = 78\n",
    "\n",
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Dataloader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrawTexModel(\n",
      "  (relu): ReLU()\n",
      "  (conv1): Conv2d(1, 150, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "  (conv1_bn): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(150, 200, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "  (conv2_bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(200, 300, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "  (conv3_bn): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(300, 500, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "  (conv4_bn): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(500, 800, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "  (conv5_bn): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin1): Linear(in_features=20000, out_features=78, bias=False)\n",
      "  (lin1_bn): BatchNorm1d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_set = DrawtexDataset(transforms.ToTensor())\n",
    "TRAIN_SIZE = int(0.8 * len(data_set))\n",
    "TEST_SIZE = len(data_set) - TRAIN_SIZE\n",
    "train_set, test_set = torch.utils.data.random_split(data_set, [TRAIN_SIZE, TEST_SIZE])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=TRAIN_BATCH_SZ,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=TEST_BATCH_SZ,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "model = DrawTexModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000,  1.0000,  0.9998, -0.8419, -0.9999, -0.9998, -0.9998,  0.9999,\n",
      "         -1.0000,  0.9999,  1.0000,  0.9990, -1.0000,  0.9017,  0.9996, -0.9999,\n",
      "         -0.9999,  0.9998,  0.9998, -1.0000,  0.9991,  0.9999,  1.0000, -0.9999,\n",
      "          0.9999,  0.9999, -0.9999,  0.9998,  1.0000,  0.9938,  0.9998,  0.9999,\n",
      "         -1.0000,  0.9988,  0.9997, -0.9954, -0.9998, -0.9995,  0.9999, -0.9995,\n",
      "         -0.9995,  0.9998,  0.9999, -1.0000,  0.9999,  0.9995, -1.0000, -0.9999,\n",
      "          0.9998, -0.9973,  0.9999,  0.9999, -1.0000,  0.9989,  1.0000, -1.0000,\n",
      "          0.9951, -1.0000,  0.9986,  0.9999, -0.9999, -0.9999,  1.0000,  1.0000,\n",
      "         -0.9977,  0.9992, -1.0000, -0.9984,  1.0000,  0.9839,  0.9999, -0.9998,\n",
      "          0.9999,  1.0000, -0.9990,  1.0000, -0.9994, -0.9999],\n",
      "        [ 1.0000, -1.0000, -0.9998,  0.8419,  0.9999,  0.9998,  0.9998, -0.9999,\n",
      "          1.0000, -0.9999, -1.0000, -0.9990,  1.0000, -0.9017, -0.9996,  0.9999,\n",
      "          0.9999, -0.9998, -0.9998,  1.0000, -0.9991, -0.9999, -1.0000,  0.9999,\n",
      "         -0.9999, -0.9999,  0.9999, -0.9998, -1.0000, -0.9938, -0.9998, -0.9999,\n",
      "          1.0000, -0.9988, -0.9997,  0.9954,  0.9998,  0.9995, -0.9999,  0.9995,\n",
      "          0.9995, -0.9998, -0.9999,  1.0000, -0.9999, -0.9995,  1.0000,  0.9999,\n",
      "         -0.9998,  0.9973, -0.9999, -0.9999,  1.0000, -0.9989, -1.0000,  1.0000,\n",
      "         -0.9951,  1.0000, -0.9986, -0.9999,  0.9999,  0.9999, -1.0000, -1.0000,\n",
      "          0.9977, -0.9992,  1.0000,  0.9984, -1.0000, -0.9839, -0.9999,  0.9998,\n",
      "         -0.9999, -1.0000,  0.9990, -1.0000,  0.9994,  0.9999]],\n",
      "       device='cuda:0', grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tens = torch.rand((2, 1, 45, 45)).to(device)\n",
    "\n",
    "output1 = model(tens)\n",
    "\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 1/2324, Loss 4.8679\n",
      "Epoch 1/1, Batch 11/2324, Loss 2.9351\n",
      "Epoch 1/1, Batch 21/2324, Loss 2.7233\n",
      "Epoch 1/1, Batch 31/2324, Loss 2.2919\n",
      "Epoch 1/1, Batch 41/2324, Loss 2.2890\n",
      "Epoch 1/1, Batch 51/2324, Loss 2.1070\n",
      "Epoch 1/1, Batch 61/2324, Loss 2.0257\n",
      "Epoch 1/1, Batch 71/2324, Loss 1.9609\n",
      "Epoch 1/1, Batch 81/2324, Loss 1.7730\n",
      "Epoch 1/1, Batch 91/2324, Loss 1.9109\n",
      "Epoch 1/1, Batch 101/2324, Loss 1.4832\n",
      "Epoch 1/1, Batch 111/2324, Loss 1.4727\n",
      "Epoch 1/1, Batch 121/2324, Loss 1.5127\n",
      "Epoch 1/1, Batch 131/2324, Loss 1.5213\n",
      "Epoch 1/1, Batch 141/2324, Loss 1.4244\n",
      "Epoch 1/1, Batch 151/2324, Loss 1.4434\n",
      "Epoch 1/1, Batch 161/2324, Loss 1.3736\n",
      "Epoch 1/1, Batch 171/2324, Loss 1.3135\n",
      "Epoch 1/1, Batch 181/2324, Loss 1.3706\n",
      "Epoch 1/1, Batch 191/2324, Loss 1.3479\n",
      "Epoch 1/1, Batch 201/2324, Loss 1.1961\n",
      "Epoch 1/1, Batch 211/2324, Loss 1.2068\n",
      "Epoch 1/1, Batch 221/2324, Loss 1.1593\n",
      "Epoch 1/1, Batch 231/2324, Loss 1.2232\n",
      "Epoch 1/1, Batch 241/2324, Loss 1.2027\n",
      "Epoch 1/1, Batch 251/2324, Loss 1.2854\n",
      "Epoch 1/1, Batch 261/2324, Loss 1.0774\n",
      "Epoch 1/1, Batch 271/2324, Loss 1.0833\n",
      "Epoch 1/1, Batch 281/2324, Loss 1.2040\n",
      "Epoch 1/1, Batch 291/2324, Loss 1.0112\n",
      "Epoch 1/1, Batch 301/2324, Loss 1.0433\n",
      "Epoch 1/1, Batch 311/2324, Loss 1.0167\n",
      "Epoch 1/1, Batch 321/2324, Loss 1.0556\n",
      "Epoch 1/1, Batch 331/2324, Loss 1.0361\n",
      "Epoch 1/1, Batch 341/2324, Loss 0.9501\n",
      "Epoch 1/1, Batch 351/2324, Loss 0.8806\n",
      "Epoch 1/1, Batch 361/2324, Loss 0.9374\n",
      "Epoch 1/1, Batch 371/2324, Loss 0.9303\n",
      "Epoch 1/1, Batch 381/2324, Loss 0.8170\n",
      "Epoch 1/1, Batch 391/2324, Loss 0.7282\n",
      "Epoch 1/1, Batch 401/2324, Loss 0.7144\n",
      "Epoch 1/1, Batch 411/2324, Loss 0.6959\n",
      "Epoch 1/1, Batch 421/2324, Loss 0.7872\n",
      "Epoch 1/1, Batch 431/2324, Loss 0.8088\n",
      "Epoch 1/1, Batch 441/2324, Loss 0.7188\n",
      "Epoch 1/1, Batch 451/2324, Loss 0.7913\n",
      "Epoch 1/1, Batch 461/2324, Loss 0.7327\n",
      "Epoch 1/1, Batch 471/2324, Loss 0.7678\n",
      "Epoch 1/1, Batch 481/2324, Loss 0.7962\n",
      "Epoch 1/1, Batch 491/2324, Loss 0.6674\n",
      "Epoch 1/1, Batch 501/2324, Loss 0.8821\n",
      "Epoch 1/1, Batch 511/2324, Loss 0.6320\n",
      "Epoch 1/1, Batch 521/2324, Loss 0.7059\n",
      "Epoch 1/1, Batch 531/2324, Loss 0.7513\n",
      "Epoch 1/1, Batch 541/2324, Loss 0.6388\n",
      "Epoch 1/1, Batch 551/2324, Loss 0.8110\n",
      "Epoch 1/1, Batch 561/2324, Loss 0.6771\n",
      "Epoch 1/1, Batch 571/2324, Loss 0.6044\n",
      "Epoch 1/1, Batch 581/2324, Loss 0.6634\n",
      "Epoch 1/1, Batch 591/2324, Loss 0.7330\n",
      "Epoch 1/1, Batch 601/2324, Loss 0.7624\n",
      "Epoch 1/1, Batch 611/2324, Loss 0.5940\n",
      "Epoch 1/1, Batch 621/2324, Loss 0.7479\n",
      "Epoch 1/1, Batch 631/2324, Loss 0.7310\n",
      "Epoch 1/1, Batch 641/2324, Loss 0.6386\n",
      "Epoch 1/1, Batch 651/2324, Loss 0.6004\n",
      "Epoch 1/1, Batch 661/2324, Loss 0.6743\n",
      "Epoch 1/1, Batch 671/2324, Loss 0.6518\n",
      "Epoch 1/1, Batch 681/2324, Loss 0.5787\n",
      "Epoch 1/1, Batch 691/2324, Loss 0.5187\n",
      "Epoch 1/1, Batch 701/2324, Loss 0.4921\n",
      "Epoch 1/1, Batch 711/2324, Loss 0.6232\n",
      "Epoch 1/1, Batch 721/2324, Loss 0.7381\n",
      "Epoch 1/1, Batch 731/2324, Loss 0.5489\n",
      "Epoch 1/1, Batch 741/2324, Loss 0.5994\n",
      "Epoch 1/1, Batch 751/2324, Loss 0.7058\n",
      "Epoch 1/1, Batch 761/2324, Loss 0.4788\n",
      "Epoch 1/1, Batch 771/2324, Loss 0.6373\n",
      "Epoch 1/1, Batch 781/2324, Loss 0.5481\n",
      "Epoch 1/1, Batch 791/2324, Loss 0.7149\n",
      "Epoch 1/1, Batch 801/2324, Loss 0.5109\n",
      "Epoch 1/1, Batch 811/2324, Loss 0.5314\n",
      "Epoch 1/1, Batch 821/2324, Loss 0.5955\n",
      "Epoch 1/1, Batch 831/2324, Loss 0.4876\n",
      "Epoch 1/1, Batch 841/2324, Loss 0.5071\n",
      "Epoch 1/1, Batch 851/2324, Loss 0.5907\n",
      "Epoch 1/1, Batch 861/2324, Loss 0.4858\n",
      "Epoch 1/1, Batch 871/2324, Loss 0.4713\n",
      "Epoch 1/1, Batch 881/2324, Loss 0.5171\n",
      "Epoch 1/1, Batch 891/2324, Loss 0.5492\n",
      "Epoch 1/1, Batch 901/2324, Loss 0.6815\n",
      "Epoch 1/1, Batch 911/2324, Loss 0.5716\n",
      "Epoch 1/1, Batch 921/2324, Loss 0.3865\n",
      "Epoch 1/1, Batch 931/2324, Loss 0.4897\n",
      "Epoch 1/1, Batch 941/2324, Loss 0.3789\n",
      "Epoch 1/1, Batch 951/2324, Loss 0.4279\n",
      "Epoch 1/1, Batch 961/2324, Loss 0.5690\n",
      "Epoch 1/1, Batch 971/2324, Loss 0.4054\n",
      "Epoch 1/1, Batch 981/2324, Loss 0.3848\n",
      "Epoch 1/1, Batch 991/2324, Loss 0.5067\n",
      "Epoch 1/1, Batch 1001/2324, Loss 0.5225\n",
      "Epoch 1/1, Batch 1011/2324, Loss 0.4796\n",
      "Epoch 1/1, Batch 1021/2324, Loss 0.5977\n",
      "Epoch 1/1, Batch 1031/2324, Loss 0.5026\n",
      "Epoch 1/1, Batch 1041/2324, Loss 0.4544\n",
      "Epoch 1/1, Batch 1051/2324, Loss 0.4130\n",
      "Epoch 1/1, Batch 1061/2324, Loss 0.4185\n",
      "Epoch 1/1, Batch 1071/2324, Loss 0.4115\n",
      "Epoch 1/1, Batch 1081/2324, Loss 0.3241\n",
      "Epoch 1/1, Batch 1091/2324, Loss 0.4398\n",
      "Epoch 1/1, Batch 1101/2324, Loss 0.3843\n",
      "Epoch 1/1, Batch 1111/2324, Loss 0.4505\n",
      "Epoch 1/1, Batch 1121/2324, Loss 0.4603\n",
      "Epoch 1/1, Batch 1131/2324, Loss 0.3593\n",
      "Epoch 1/1, Batch 1141/2324, Loss 0.4164\n",
      "Epoch 1/1, Batch 1151/2324, Loss 0.4866\n",
      "Epoch 1/1, Batch 1161/2324, Loss 0.3678\n",
      "Epoch 1/1, Batch 1171/2324, Loss 0.5279\n",
      "Epoch 1/1, Batch 1181/2324, Loss 0.3891\n",
      "Epoch 1/1, Batch 1191/2324, Loss 0.3285\n",
      "Epoch 1/1, Batch 1201/2324, Loss 0.4923\n",
      "Epoch 1/1, Batch 1211/2324, Loss 0.5259\n",
      "Epoch 1/1, Batch 1221/2324, Loss 0.3436\n",
      "Epoch 1/1, Batch 1231/2324, Loss 0.4043\n",
      "Epoch 1/1, Batch 1241/2324, Loss 0.5033\n",
      "Epoch 1/1, Batch 1251/2324, Loss 0.5091\n",
      "Epoch 1/1, Batch 1261/2324, Loss 0.3651\n",
      "Epoch 1/1, Batch 1271/2324, Loss 0.3107\n",
      "Epoch 1/1, Batch 1281/2324, Loss 0.4487\n",
      "Epoch 1/1, Batch 1291/2324, Loss 0.4104\n",
      "Epoch 1/1, Batch 1301/2324, Loss 0.4326\n",
      "Epoch 1/1, Batch 1311/2324, Loss 0.3912\n",
      "Epoch 1/1, Batch 1321/2324, Loss 0.3221\n",
      "Epoch 1/1, Batch 1331/2324, Loss 0.2931\n",
      "Epoch 1/1, Batch 1341/2324, Loss 0.3426\n",
      "Epoch 1/1, Batch 1351/2324, Loss 0.4084\n",
      "Epoch 1/1, Batch 1361/2324, Loss 0.3549\n",
      "Epoch 1/1, Batch 1371/2324, Loss 0.3350\n",
      "Epoch 1/1, Batch 1381/2324, Loss 0.3635\n",
      "Epoch 1/1, Batch 1391/2324, Loss 0.3687\n",
      "Epoch 1/1, Batch 1401/2324, Loss 0.4088\n",
      "Epoch 1/1, Batch 1411/2324, Loss 0.4172\n",
      "Epoch 1/1, Batch 1421/2324, Loss 0.3823\n",
      "Epoch 1/1, Batch 1431/2324, Loss 0.4179\n",
      "Epoch 1/1, Batch 1441/2324, Loss 0.3346\n",
      "Epoch 1/1, Batch 1451/2324, Loss 0.4023\n",
      "Epoch 1/1, Batch 1461/2324, Loss 0.3403\n",
      "Epoch 1/1, Batch 1471/2324, Loss 0.4500\n",
      "Epoch 1/1, Batch 1481/2324, Loss 0.3586\n",
      "Epoch 1/1, Batch 1491/2324, Loss 0.4588\n",
      "Epoch 1/1, Batch 1501/2324, Loss 0.4523\n",
      "Epoch 1/1, Batch 1511/2324, Loss 0.4017\n",
      "Epoch 1/1, Batch 1521/2324, Loss 0.4142\n",
      "Epoch 1/1, Batch 1531/2324, Loss 0.4507\n",
      "Epoch 1/1, Batch 1541/2324, Loss 0.3911\n",
      "Epoch 1/1, Batch 1551/2324, Loss 0.2697\n",
      "Epoch 1/1, Batch 1561/2324, Loss 0.3356\n",
      "Epoch 1/1, Batch 1571/2324, Loss 0.3744\n",
      "Epoch 1/1, Batch 1581/2324, Loss 0.4014\n",
      "Epoch 1/1, Batch 1591/2324, Loss 0.2368\n",
      "Epoch 1/1, Batch 1601/2324, Loss 0.2524\n",
      "Epoch 1/1, Batch 1611/2324, Loss 0.2678\n",
      "Epoch 1/1, Batch 1621/2324, Loss 0.3369\n",
      "Epoch 1/1, Batch 1631/2324, Loss 0.3297\n",
      "Epoch 1/1, Batch 1641/2324, Loss 0.3080\n",
      "Epoch 1/1, Batch 1651/2324, Loss 0.3193\n",
      "Epoch 1/1, Batch 1661/2324, Loss 0.4130\n",
      "Epoch 1/1, Batch 1671/2324, Loss 0.3363\n",
      "Epoch 1/1, Batch 1681/2324, Loss 0.2423\n",
      "Epoch 1/1, Batch 1691/2324, Loss 0.2766\n",
      "Epoch 1/1, Batch 1701/2324, Loss 0.3957\n",
      "Epoch 1/1, Batch 1711/2324, Loss 0.2153\n",
      "Epoch 1/1, Batch 1721/2324, Loss 0.3254\n",
      "Epoch 1/1, Batch 1731/2324, Loss 0.3822\n",
      "Epoch 1/1, Batch 1741/2324, Loss 0.2870\n",
      "Epoch 1/1, Batch 1751/2324, Loss 0.3613\n",
      "Epoch 1/1, Batch 1761/2324, Loss 0.2807\n",
      "Epoch 1/1, Batch 1771/2324, Loss 0.2913\n",
      "Epoch 1/1, Batch 1781/2324, Loss 0.3048\n",
      "Epoch 1/1, Batch 1791/2324, Loss 0.2856\n",
      "Epoch 1/1, Batch 1801/2324, Loss 0.3540\n",
      "Epoch 1/1, Batch 1811/2324, Loss 0.3498\n",
      "Epoch 1/1, Batch 1821/2324, Loss 0.3065\n",
      "Epoch 1/1, Batch 1831/2324, Loss 0.3315\n",
      "Epoch 1/1, Batch 1841/2324, Loss 0.3018\n",
      "Epoch 1/1, Batch 1851/2324, Loss 0.3968\n",
      "Epoch 1/1, Batch 1861/2324, Loss 0.2765\n",
      "Epoch 1/1, Batch 1871/2324, Loss 0.3622\n",
      "Epoch 1/1, Batch 1881/2324, Loss 0.1986\n",
      "Epoch 1/1, Batch 1891/2324, Loss 0.2166\n",
      "Epoch 1/1, Batch 1901/2324, Loss 0.3440\n",
      "Epoch 1/1, Batch 1911/2324, Loss 0.3472\n",
      "Epoch 1/1, Batch 1921/2324, Loss 0.2771\n",
      "Epoch 1/1, Batch 1931/2324, Loss 0.2730\n",
      "Epoch 1/1, Batch 1941/2324, Loss 0.2938\n",
      "Epoch 1/1, Batch 1951/2324, Loss 0.3026\n",
      "Epoch 1/1, Batch 1961/2324, Loss 0.3154\n",
      "Epoch 1/1, Batch 1971/2324, Loss 0.2005\n",
      "Epoch 1/1, Batch 1981/2324, Loss 0.3204\n",
      "Epoch 1/1, Batch 1991/2324, Loss 0.2926\n",
      "Epoch 1/1, Batch 2001/2324, Loss 0.2262\n",
      "Epoch 1/1, Batch 2011/2324, Loss 0.3132\n",
      "Epoch 1/1, Batch 2021/2324, Loss 0.2577\n",
      "Epoch 1/1, Batch 2031/2324, Loss 0.2998\n",
      "Epoch 1/1, Batch 2041/2324, Loss 0.3161\n",
      "Epoch 1/1, Batch 2051/2324, Loss 0.2471\n",
      "Epoch 1/1, Batch 2061/2324, Loss 0.2127\n",
      "Epoch 1/1, Batch 2071/2324, Loss 0.2565\n",
      "Epoch 1/1, Batch 2081/2324, Loss 0.2568\n",
      "Epoch 1/1, Batch 2091/2324, Loss 0.3227\n",
      "Epoch 1/1, Batch 2101/2324, Loss 0.3264\n",
      "Epoch 1/1, Batch 2111/2324, Loss 0.2978\n",
      "Epoch 1/1, Batch 2121/2324, Loss 0.2533\n",
      "Epoch 1/1, Batch 2131/2324, Loss 0.3186\n",
      "Epoch 1/1, Batch 2141/2324, Loss 0.3412\n",
      "Epoch 1/1, Batch 2151/2324, Loss 0.2473\n",
      "Epoch 1/1, Batch 2161/2324, Loss 0.2426\n",
      "Epoch 1/1, Batch 2171/2324, Loss 0.2960\n",
      "Epoch 1/1, Batch 2181/2324, Loss 0.2453\n",
      "Epoch 1/1, Batch 2191/2324, Loss 0.3159\n",
      "Epoch 1/1, Batch 2201/2324, Loss 0.2478\n",
      "Epoch 1/1, Batch 2211/2324, Loss 0.3360\n",
      "Epoch 1/1, Batch 2221/2324, Loss 0.2450\n",
      "Epoch 1/1, Batch 2231/2324, Loss 0.1798\n",
      "Epoch 1/1, Batch 2241/2324, Loss 0.3249\n",
      "Epoch 1/1, Batch 2251/2324, Loss 0.1988\n",
      "Epoch 1/1, Batch 2261/2324, Loss 0.1957\n",
      "Epoch 1/1, Batch 2271/2324, Loss 0.2027\n",
      "Epoch 1/1, Batch 2281/2324, Loss 0.2481\n",
      "Epoch 1/1, Batch 2291/2324, Loss 0.2174\n",
      "Epoch 1/1, Batch 2301/2324, Loss 0.2404\n",
      "Epoch 1/1, Batch 2311/2324, Loss 0.1702\n",
      "Epoch 1/1, Batch 2321/2324, Loss 0.2065\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n",
    "    steps = len(train_loader)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (img, label) in enumerate(train_loader):\n",
    "            img: torch.Tensor = img.to(device, non_blocking=True)\n",
    "            label: torch.Tensor = label.to(device, non_blocking=True)\n",
    "            output = model(img).to(device)\n",
    "            loss: torch.Tensor = criterion(output, label).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{EPOCHS}, Batch {i + 1}/{steps}, Loss {loss.item():.4f}\")\n",
    "    torch.save(model.state_dict(), \"./DrawTexModel.pth\")\n",
    "    torch.save(optimizer.state_dict(), \"./Optimizer.pth\")\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.64465632942886%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for img, labels in test_loader:\n",
    "        img = img.to(device, non_blocking= True)\n",
    "        labels = labels.to(device, non_blocking= True)\n",
    "\n",
    "        output = model(img)\n",
    "        _, prediction = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (prediction == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    print(f\"Accuracy: {acc}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc82db2dc128658da88fa564359fe123ee9c6914f8c7963941acb47815200485"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
